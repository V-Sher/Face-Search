{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "source": [
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.neighbors import NearestNeighbors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Insightface Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "app = FaceAnalysis(name=\"antelope\")\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input mean and std: 127.5 127.5\n",
      "find model: /Users/varshita/.insightface/models/antelope/glintr100.onnx recognition\n",
      "find model: /Users/varshita/.insightface/models/antelope/scrfd_10g_bnkps.onnx detection\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "`The Yale Face Database (size 6.4MB) contains 165 grayscale images in GIF format of 15 individuals. There are 11 images per subject, one per different facial expression or configuration: center-light, w/glasses, happy, left-light, w/no glasses, normal, right-light, sad, sleepy, surprised, and wink.`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Fixing the file extensions\n",
    "YALE_DIR = \"../data/yalefaces\"\n",
    "files = os.listdir(YALE_DIR)[1:]\n",
    "for i, img in enumerate(files):\n",
    "    # print(\"original name: \", img)\n",
    "    new_ext_name = \"_\".join(img.split(\".\")) + \".gif\"\n",
    "    # print(\"new name: \",  new_ext_name)\n",
    "    os.rename(os.path.join(YALE_DIR, img), os.path.join(YALE_DIR, new_ext_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "source": [
    "def create_probe_eval_set(files: List):\n",
    "    # pick random index between 0 and len(files)-1\n",
    "    random_idx = np.random.randint(0,len(files))\n",
    "    probe_img_fpaths = [files[random_idx]]\n",
    "    eval_img_fpaths = [files[idx] for idx in range(len(files)) if idx != random_idx]\n",
    "    \n",
    "    return probe_img_fpaths, eval_img_fpaths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "def generate_embs(img_fpaths: List[str]):\n",
    "    embs_set = list()\n",
    "    embs_label = list()\n",
    "\n",
    "    for img_fpath in img_fpaths:  \n",
    "                    \n",
    "        # read grayscale img\n",
    "        img = Image.open(os.path.join(YALE_DIR, img_fpath)) \n",
    "        img_arr = np.asarray(img)  \n",
    "        \n",
    "        # convert grayscale to rgb\n",
    "        im = Image.fromarray((img_arr * 255).astype(np.uint8))\n",
    "        rgb_arr = np.asarray(im.convert('RGB'))       \n",
    "       \n",
    "        # generate Insightface embedding\n",
    "        res = app.get(rgb_arr)          \n",
    "        # append emb to the eval set\n",
    "        embs_set.append(res)          \n",
    "        # append label to eval_label set\n",
    "        embs_label.append(img_fpath.split(\"_\")[0])          \n",
    "\n",
    "    return embs_set, embs_label\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "source": [
    "def filter_empty_embs(img_set: List, img_labels: List[str]):\n",
    "    # filtering where insightface could not generate an embedding\n",
    "    good_idx = [i for i,x in enumerate(img_set) if x]\n",
    "    \n",
    "    if len(good_idx) == len(img_set):\n",
    "        clean_embs = [e[0].embedding for e in img_set]\n",
    "        clean_labels = img_labels\n",
    "        \n",
    "    else:\n",
    "        # filtering eval set and labels based on good idx\n",
    "        clean_labels = np.array(img_labels)[good_idx]\n",
    "        clean_set = np.array(img_set, dtype=object)[good_idx]\n",
    "        \n",
    "        # generating embs for good idx\n",
    "        clean_embs = [e[0].embedding for e in clean_set]\n",
    "    \n",
    "    return clean_embs, clean_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate probe and eval set embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "source": [
    "# sorting files\n",
    "files = os.listdir(YALE_DIR)\n",
    "files.sort()\n",
    "eval_set = list()\n",
    "eval_labels = list()\n",
    "probe_set = list()\n",
    "probe_labels = list()\n",
    "IMAGES_PER_IDENTITY = 11\n",
    "for i in tqdm(range(1, len(files), IMAGES_PER_IDENTITY), unit_divisor=True): # ignore the README.txt file at files[0]\n",
    "    # print(i)\n",
    "    probe, eval = create_probe_eval_set(files[i:i+IMAGES_PER_IDENTITY])\n",
    "    \n",
    "    # store eval embs and labels\n",
    "    eval_set_t, eval_labels_t = generate_embs(eval)\n",
    "    eval_set.extend(eval_set_t)\n",
    "    eval_labels.extend(eval_labels_t)\n",
    "    \n",
    "    # store probe embs and labels\n",
    "    probe_set_t, probe_labels_t = generate_embs(probe)\n",
    "    probe_set.extend(probe_set_t)\n",
    "    probe_labels.extend(probe_labels_t)\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 15/15 [01:04<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "source": [
    "assert len(eval_set) == len(eval_labels)\n",
    "assert len(probe_set) == len(probe_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "source": [
    "evaluation_embs, evaluation_labels = filter_empty_embs(eval_set, eval_labels)\n",
    "probe_embs, probe_labels = filter_empty_embs(probe_set, probe_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "source": [
    "assert len(evaluation_embs) == len(evaluation_labels)\n",
    "assert len(probe_embs) == len(probe_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train K NearestNeighbours"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "source": [
    "# Train KNN classifier\n",
    "nn = NearestNeighbors(n_neighbors=3, metric=\"cosine\")\n",
    "nn.fit(X=evaluation_embs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NearestNeighbors(metric='cosine', n_neighbors=3)"
      ]
     },
     "metadata": {},
     "execution_count": 329
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Optional - saving and loading model\n",
    "# save the model to disk\n",
    "filename = 'faceID_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(nn, file)\n",
    "    \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "# with open(filename, 'rb') as file:\n",
    "#     pickle_model = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dists, inds = nn.kneighbors(X=probe_embs, n_neighbors=2, return_distance=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating metrics - p_at_k"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "source": [
    "# p@k\n",
    "p_at_k = np.zeros(len(probe_embs))\n",
    "for i in range(len(probe_embs)):\n",
    "    true_label = probe_labels[i]\n",
    "    pred_neighbr_idx = inds[i]\n",
    "    \n",
    "    pred_labels = [evaluation_labels[id] for id in pred_neighbr_idx]\n",
    "    pred_is_labels = [1 if label == true_label else 0 for label in pred_labels]\n",
    "    \n",
    "    p_at_k[i] = np.mean(pred_is_labels)\n",
    "    \n",
    "p_at_k.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "metadata": {},
     "execution_count": 330
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running Face ID for unknown faces"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "source": [
    "def print_ID_results(img_fpath: str, evaluation_labels: np.ndarray, verbose: bool = False):\n",
    "    img = Image.open(img_fpath)\n",
    "    img_emb = app.get(np.asarray(img))[0].embedding\n",
    "    \n",
    "    # get pred from KNN\n",
    "    dists, inds = nn.kneighbors(X=img_emb.reshape(1,-1), n_neighbors=3, return_distance=True)\n",
    "    \n",
    "    # get labels of the neighbours\n",
    "    pred_labels = [evaluation_labels[i] for i in inds[0]]\n",
    "    \n",
    "    # check if any dist is greater than 0.5, and if so, print the results\n",
    "    no_of_matching_faces = np.sum([1 if d <=0.6 else 0 for d in dists[0]])\n",
    "    if no_of_matching_faces > 0:\n",
    "        print(\"Matching face(s) found in database! \")\n",
    "        verbose = True\n",
    "    else: \n",
    "        print(\"No matching face(s) not found in database!\")\n",
    "        \n",
    "    # print labels and corresponding distances\n",
    "    if verbose:\n",
    "        for label, dist in zip(pred_labels, dists[0]):\n",
    "            print(f\"Nearest neighbours found in the database have labels {label} and is at a distance of {dist}\")\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "source": [
    "print_ID_results(\"../data/baby4.jpg\", evaluation_labels, verbose=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No matching face(s) not found in database!\n",
      "Nearest neighbours found in the database have labels subject02 and is at a distance of 0.7562326192855835\n",
      "Nearest neighbours found in the database have labels subject02 and is at a distance of 0.9153403043746948\n",
      "Nearest neighbours found in the database have labels subject01 and is at a distance of 0.9396535158157349\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('face_search_env': venv)"
  },
  "interpreter": {
   "hash": "763c1bb747ca7dcb4748acbf9dcb37cd6e11f816709b8f1b372e49d0ab504df1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}